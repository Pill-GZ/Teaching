%
% This is the LaTeX template file for lecture notes for STATS700,
% Bayesian Inference and Computations.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%
% This template is based on the template for Prof. Sinclair's CS 270.

\documentclass[twoside]{article}
\usepackage{amsmath} % math
\usepackage{mathtools} % math
\usepackage{amssymb} % big real number R
\usepackage{graphics}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf STATS 700 Bayesian Inference and Computation \hfill Fall 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
   {\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications.  They may be distributed
   outside this class only with the permission of the Instructor.}
   \vspace*{4mm}
}


%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{5}{Oct 4 Hierarchical Models}{CHEN Yang}{Po-Heng Chen}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

% \section{Constructing a parameterized prior distribution}
% \textbf{Analysis with fixed prior distribution} 

% Currently we have the following data, and we can use these to make inference with what we learned from Single Parameter Model.

% $observations$: 4 out of 14 have cancer \\
% $prior$: Beta($\alpha, \beta$) \\
% $posterior$: Beta($\alpha + 4, \beta + 10$) 

% \textbf{Approximate estimate using historical data}

% $historical$ $observations$: ($y_1, n_1$), ($y_2, n_2$), ..., ($y_{71}, n_{71}$)
% $prior$: 

\section{Improper Prior}
Setting up an improper prior (or non-informative prior) could be very dangerous to use in practice or in general problem because if you set up such kind of prior and you cannot check this condition. Very likely you will have an improper prior distribution and, whatever your inference is, it will be invalid because you are not making inference from a proper prior. 

A general recommendation in practice is to set up a proper prior. For example, it is a positive value such as Gamma prior.

\section{Exchangeability}
Exchangeability means we do not have any information other than the data about how to distinguish the different parameters. In \textbf{rat tumor} example, it's we don't have information to distinguish the rats from different labs. We don't have the information to distinguish what is the rat tumor probability from different rats.

Formal mathematical definition:
The parameters $(\theta_1, ... ,\theta_J)$ are exchangeable if the density $p(\theta_1, ..., \theta_J)$ is invariant to permutations of the indexes $(1,...,J)$. 

Simple form:
\begin{align*}
    p(\theta) = \int \bigg( \prod_{j=1}^{J} p(\theta_j|\phi) \bigg) p(\phi) d\phi
\end{align*}

\textbf{de Finetti's theorem}\\
If you have some exchangeable distribution, you will be able to express it in terms of i.i.d observations and then adding a prior on the parameters.


\section{Gaussian Example}


Observations' distributions: $\theta_1, \theta_2, ... ,\theta_j$ where $\theta$ are Gaussian means.\\
Observations: $y_{1j}, y_{2j}, ... y_{nj}$ for $\theta_j$

We also assume that all of the $\theta$ values come from a global distribution, which is another Gaussian distribution with $(\mu, \tau^2)$. Then we can further assume a prior $P_0(\mu, \tau^2)$ for $N(\mu, \tau^2)$. 

This is a hierarchical model where the observations at the bottom, parameters at the middle and hyper-parameters at the top.

\textbf{How to make inference form this model?}

Since $y_j$ follows $N(\theta_j, \sigma^2)$, $\bar{y}_j = \frac{\sum_{j=1}^{n_j} y_j}{n_j}$ follows $N(\theta_j, \frac{\sigma^2}{n_j})$.

For each group, we try to calculate what is the sample mean. Because if we forget about the hierarchical model, a very natural estimate of $\theta_j$ is the $\bar{y}_j$. 

We can simplify the observations using summery statistics:
\begin{align*}
    \theta_1  \rightarrow \bar{y}_1 & \sim N(\theta_1, \frac{n_1}{\sigma^2}) \\ 
    \theta_2  \rightarrow \bar{y}_2 & \sim N(\theta_2, \frac{n_2}{\sigma^2}) \\ 
    & \dots \\ 
    \theta_j  \rightarrow \bar{y}_j & \sim N(\theta_j, \frac{n_j}{\sigma^2})
\end{align*}

Then we are able to write down the joint distribution of $\mu$, $\tau^2$, all the $\theta_J$, conditioning on the observations:
\begin{align*}
      p(\mu, \tau^2, \big\{\theta_i \big\} ^{J}_{i=1} | \bar{y}_1, ... \bar{y}_j) & = p(\mu, \tau^2) \prod_{i=1}^{J} p(\theta_i|\mu, \tau^2) \prod_{i=1}^{J} p(\bar{y}_i|\theta_i, \sigma_i^2) \\
    & =  p(\mu, \tau^2) \prod_{i=1}^{J} \big\{ p(\theta_i|\mu, \tau^2) p(\bar{y}_i|\theta_i, \sigma_i^2) \big\}
\end{align*}
If we know $p(\mu, \tau^2 | observations)$, posterior of $\theta_i$ is $N\big( \frac{\frac{\mu}{\tau^2}+\frac{\bar{y}_i}{\sigma^2}}{\frac{1}{\tau^2}+\frac{1}{\sigma_i^2}}, \frac{1}{\frac{1}{\tau^2}+\frac{1}{\sigma_i^2}}\big)$.
\begin{align*}
      p(\mu, \tau^2 | \bar{y}_1,...,\bar{y}_j) 
      & = \int p(\mu, \tau^2, \big\{\theta_i \big\} ^{J}_{i=1} | \bar{y}_1, ... \bar{y}_j) d\theta_1...d\theta_J \\
      & = p(\mu, \tau^2) \prod_{i=1}^{J} \int p(\theta_i|\mu, \tau^2) p(\bar{y}_i | \theta_i, \sigma_i^2) d\theta_i
\end{align*}


% A hierarchical model includes two parts:
%   - parameter level: observations' priors and their priors
%   - observation level: observations and their priors
% Let's assume here is a two-level hierarchical model whose parameter level includes the observations' prior $N(\mu, \tau^2)$ and the prior of the observations' prior $P_0(\mu, \tau^2)$. $P_0(\mu, \tau^2)$ is the prior distribution of $N(\mu, \tau^2)$. $N(\mu, \tau^2)$ is the prior distribution of $\theta_1, \theta_2, ..., \theta_j$ which follows the Normal distribution with $(\theta_j, \sigma^2)$. And $\theta_j$ is the distribution of the observations $y_{1j}, y_{2j}, ..., y_{nj}$.\\



\section*{References}

\begin{thebibliography}{}

\bibitem{efron2012large}
Bradley Efron.
\newblock {\em Large-scale inference: empirical Bayes methods for estimation,
  testing, and prediction}, Volume~1.
\newblock Cambridge University Press, 2012.


\bibitem{gelman2014bayesian}
Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin.
\newblock {\em Bayesian data analysis}, Volume~2.
\newblock CRC press Boca Raton, FL, 2014.

\end{thebibliography}


\end{document}





