---
title: "Logistic Regression, Model Selection, and Cross Validation"
author: "GAO Zheng"
date: "March 25, 2017"
output:
  html_document: 
    number_sections: false
    css: "~/Teaching/markdown7.css"
  html_notebook: default
---

Dataset for this tutorial can be found here:
[LoanStatsTraining.csv](../data/LoanStatsTraining.csv)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Classification problems

In this project we are trying to predict if a loan will be in good standing or go bad, given information about the loan and the borrower. The problem is a classical classification problem; we are trying to make a binary decision, based on existing records. 

Such binary predictions can have one of four outcomes:


Classification \\ Ground Truth | Positive Sample | Negative Sample
--------------------|-----------------|-----------------
**Classified Positive** | True Positive (TP)   | False Positive (FP)
**Classified Negative** | False Negative (FN)  | True Negative (TN)

We would like to make as few mistakes as possible, i.e., make FP (type I error) and FN (type II error) small.

Different types errors carry different costs in applications. In the loan default prediction context, a bad loan misclassified as a good one may be more costly than a good loan classified as a bad one: the credit intermediary may lose all the money that it lends out in the first case, but may only suffer from a small loss of profit that they could have earned in the latter case (and possibly a small loss of goodwill).

Classification \\ Ground Truth | Bad Borrower | Good Borrower
--------------------|-----------------|-----------------
**Classified as Bad** | TP, correctly labeled   | FP, less costly mistake
**Classified as Good** | FN, more costly mistake | TN, correctly labeled


In this dataset around 86% of all records are loans in good standing. This is known as an **unbalanced** dataset, where samples of one class constitute the majority. Given this history, a naive solution to the classification problem may be to classify all incoming loan applications as good loans, and stamp approvals on all of them. This may not seem like a bad idea, after all you will get roughly 86% correct on all applications.

However this may be suboptimal: extending credit to untrust-worthy borrowers is far more risky than accidentally rejecting a good borrower. We want to be more conservative. As we move from very aggressive underwriter to an extremely conservative one, FP decreases, and FN increases. Fewer bad loans are approved, but more good loans are rejected too; costs associated with type II error decrease, costs associated with type I increase.

The best underwirter should fall somewhere in the middle: a goldilock's point where the overall cost is minimized.

In this project we assume that each approved good loan makes \$1,000 for the company, and each bad loan eventually costs the company \$5,000. Therefore the cost associate with type I and type II errors are \$1,000 and \$5,000 respectively.

## Some other measures of classification accuracy

Some most commonly used measures of accuracy are Sensitivity (also known as recall), specificity, and precision.

- **sensitivity** = TP / (TP + FN), aka true positive rate (TPR), measures how well a positive sample can be identified
- **false negative rate** (FNR) = FN / (TP + FN) = 1 - TPR, measures the proportion of true positives classified as negative.
- **specificity** = TN / (TN + FP), measures how well a negative sample can be identified
- **precision** = TP / (TP + FP), measures the proportion of true positives among all samples classified as positive

# Logistic Regression

We are going to study some tools that allow us to learn the rules to classify new observations. 
First tool we are going to look at is logistic regression.

## Motivation: A generative model

Suppose we have two batches of milk from Washtenaw Dairy, each of 50 bottles. The first batch has fat content normally distributed with mean 3.25\% and s.d. 0.5\%, the second batch has fat content 2\% and s.d. 0.5% (ignornig the negative part of the distribution). The two batches are unfortunately mixed up and we have no label on the bottles to tell them apart.

```{r}
par(mar = c(4,4,1,1))
fat.content <- seq(0, 6, 0.01)
batch1 <- dnorm(fat.content, 3.25, 0.5)
batch2 <- dnorm(fat.content, 2, 0.5)
plot(fat.content, batch1 + batch2, type = 'l', lty = 2, lwd = 2,
     xlab = "fat content (percentage points)", ylab = "density",
     ylim = c(0,0.9), yaxt='n')
polygon(c(fat.content,rev(fat.content)),
        c(batch1 + batch2,rep(0, length(fat.content))),
        col = rgb(0, 0, 1, 0.2), border = F)
polygon(c(fat.content,rev(fat.content)),
        c(batch1,rep(0, length(fat.content))),
        col = rgb(1, 0, 0, 0.5))
polygon(c(fat.content,rev(fat.content)),
        c(batch2,rep(0, length(fat.content))),
        col = rgb(0, 1, 0, 0.5))
text(x = 4, y = 0.7, "Batch 1", pos = 1)
text(x = 1.2, y = 0.7, "Batch 2", pos = 1)
text(x = 2.6, y = 0.95, "Mixture density", pos = 1)
```

Suppose we want to tell which btach a bottle is from, only by testing the fat content in the milk. We do won't have a hard time if the milk tested has fat content below 2\% or above 3.25\%; we know that the chances of us making mistakes is quite low, given the distributions. But how should we classify the milk if fat content falls somewhere in between?

Can we work out the probability of the milk being from the first batch, given its fat content?

That is,

$$
\mathrm{P}\{\text{milk from batch 1 | fat content = }x\}
$$

We can work out the ratio of density to the sum of the densities

$$
\begin{aligned}
\mathrm{P}\{\text{milk from batch 1 | fat content = }x\} &= \frac{\text{density of fat content from batch 1}}{\text{sum of densities from batch 1 and 2}} \\
 &= \frac{1/(\sqrt{2\pi}\sigma)\exp{\{-(x-\mu_1)^2/(2\sigma^2)}\}} {1/(\sqrt{2\pi}\sigma)[\exp{\{-(x-\mu_1)^2/(2\sigma^2)}\} + \exp{\{-(x-\mu_2)^2/(2\sigma^2)}\}]} \\
 &= \Big(1 + \exp{\{-\frac{2(\mu_1-\mu_2)}{2\sigma^2}x + \frac{\mu_1^2-\mu_2^2}{2\sigma^2}}\}\Big)^{-1} \\
 &= \frac{e^{ax+b}}{e^{ax+b}+1}
\end{aligned}
$$

where $a = \frac{(\mu_1-\mu_2)}{\sigma^2}$, and $b = -\frac{\mu_1^2-\mu_2^2}{2\sigma^2}$.

We can massage the algebraic relationship to get from 

$$
p = \frac{e^{ax+b}}{e^{ax+b}+1}
$$

to

$$
\log(\frac{p}{1-p}) = ax+b
$$

The function $\text{logit}(p) = \log(\frac{p}{1-p})$ is called a logit function.

We can then choose a threshold to classify objects, if a binary decision is to be made. Say we pick the cutoff at $p^*=0.5$, i.e., $\text{logit}(p^*) = 1$. Solve for $\text{logit}(p^*) = ax-b = 1$, we have $x=2.625$. So if the fat content is over than 2.625\%, we classify the bottle of milk as from batch 1.

```{r}
a <- (3.25 - 2) / 0.5^2
b <- - (3.25^2 - 2^2) / (2*0.5^2)
logistic <- exp(a*fat.content + b) / (exp(a*fat.content + b) + 1)
plot(fat.content, batch1 + batch2, type = 'l', lty = 2, lwd = 2,
     xlab = "fat content (percentage points)", ylab = "probability",
     ylim = c(0,1))
lines(fat.content, logistic, col = "blue", lwd = 2)
polygon(c(fat.content,rev(fat.content)),
        c(logistic,rep(0, length(fat.content))),
        col = rgb(1, 0, 0, 0.2), border = F)
polygon(c(fat.content,rev(fat.content)),
        c(rep(1, length(fat.content)),rev(logistic)),
        col = rgb(0, 1, 0, 0.2), border = F)
arrows(x0 = 0, y0 = 0.5, x1 = 2.625, y1 = 0.5)
abline(v = 2.625, lwd = 2, col = 6)
text(x = 3.6, y = 0.2, "Classified as batch 1", pos = 1, col = "red")
text(x = 1.6, y = 0.2, "Classified as batch 2", pos = 1, col = "darkgreen")
text(x = 4.5, y = 1, "P(batch 1| fat content)", pos = 1, col = "blue")
text(x = 1, y = 0.75, "Mixture density", pos = 1)
text(x = 0.5, y = 0.5, "p* = 0.5", pos = 1)
text(x = 1.7, y = 1, "Decision boundary", pos = 1, col = 6)
```

## Discriminative models

We usually have at hand samples with known labels, and we wish to learn the rules to make classification from the labeled samples. Think of the situation where we have previous batches of milk samples with labels of either batch 1 or 2, but we do not know the batch from which a new bottle of milk comes. Can we somehow still classify the new sample with a test of its fat content?

In practice we do not work from two known distributions to derive the conditional probabilities and classification rules. Rather we build upon the assumption that the logit function is a linear response of the covariates.


$$
\text{logit}(p) = \log(\frac{p(x)}{1-p(x)}) = a' x + b
$$

where $a$ is a vector of parameters, $b$ a scalar parameter, and $x$ the covariates.

It is important to notice that the level curves of probability are linear in the covariates.

(Typically parameters are estimated by maximizing the (log-)likelihood of our observations. Unlike linear regressions, closed form solutions do not exist for logistic regression, estimation is done via numerical optimization.)

Let's look at how this works in `R`.

We first simulate the two batches of milk, with 50 bottles each

```{r}
batch1.samples <- rnorm(100, 3.25, 0.5)
batch2.samples <- rnorm(100, 2, 0.5)
```

Create a dataframe with labels

```{r}
milk.samples <- data.frame(fat.content = c(batch1.samples, batch2.samples), label = rep(c(1,0), each = 100))
```

Then fit a logistic regression using `glm`

```{r}
(logistic.reg.fit <- glm(label ~ fat.content, family = "binomial", data = milk.samples))
```

```{r}
par(mar = c(4,4,1,4))
with(data = milk.samples, 
     expr = plot(fat.content, jitter(label), 
                 yaxt = 'n', ylab = "Prob(from batch 1)", xlab = "fat content (percentage points)",
                 pch = 8, col = rep(2:3, each = 100)))
axis(side = 4, at = 0:1, labels = c("batch 2", "batch 1"), las = 1)
b.fitted <- logistic.reg.fit$coefficients["(Intercept)"]
a.fitted <- logistic.reg.fit$coefficients["fat.content"]
logistic <- exp(a.fitted*fat.content + b.fitted) / (exp(a.fitted*fat.content + b.fitted) + 1)
lines(fat.content, logistic, col = "blue", lwd = 2)
text(x = 3.5, y = 0.8, "P(batch 1| fat content)", pos = 1, col = "blue")
axis(side = 2, at = seq(0, 1, 0.1), las = 1)

```

For new samples without labels, we can make a probablistic statement, given their fat content (say we have 2 new samples with fat content at 1.5\% and 3\%)

```{r}
(new.sample.prediction <- predict(logistic.reg.fit,
                                  newdata = data.frame(fat.content = c(1.5, 3)),
                                  type="response"))
```

i.e., the first bottle is very likely from batch 2, and the second one with high probability from batch 1.

If a binary decision were to be made, we can hard threshold the predictive probabilities to 0 an 1's

```{r}
as.numeric(new.sample.prediction > 0.5)
```


# Over/under-fitting, model selection, and Cross Validation

When we have multiple co-variates, we have the freedom of building a model that includes some or all of them.
It is natural to ask how the best model should be selected.

One may try fitting the largest model possible, but that may quickly leads to over-fitting. The some covariates that may have nothing to do with what you want to predict, but happens to correlate with your response, enter your equation. No ominous signs show up when you fit the model: the more covariates you through in, the lower the error rates on your data. Testing your model on the same set of data that you trained it on is not a good idea. When a new prediction needs to be made, a overfit model may give us way inferior results than a correctly specified one.

One way to prevent overfitting is to perform cross validation.

The idea is to 

1. partition your data into training data and testing data (sometimes called validation data); treat testing data as unobserved, and
2. fit your model using only the training data.
3. Evaluate your model on the testing data that you held out earlier, compare with the actual results, and obtain a testing error.

Repeat the process K times then take average of the testing errors as a final performance measure.

![k = 4 fold cross validation](CV.jpg)

If a model is over-crowded, then the performance will suffer in cross validation. The testing data, independent of the training data, serves as a judge for the true performance of the model.

We will demonstrate how this is done with the dataset for your project

## Illustrating cross validation with Lending Club dataset 

Suppose we want to compare prediction accuracy in terms of total cost incurred: each misclassified good loan incurs a cost of \$1,000, each misclassified bad loan incurs \$5,000, reflecting the fact that bad loan is more costly.

We first split the data into K-folds

```{r}
LCdata <- read.csv("../data/LoanStatsTraining.csv", header = T)
# scramble the indeces
LCdata <- LCdata[sample(nrow(LCdata)),]
# K = 5 fold CV
K <- 5

# demonstrate using the first split
k <- 1
fold.size <- nrow(LCdata) / K
testing.index <- (k-1)*fold.size + 1:fold.size

training <- LCdata[-testing.index,]
testing <- LCdata[testing.index,]
names(training)
# lapply(training, class) # chack all data types
```

We want to evaluate the following model

```{r}
model <- glm(formula = isBadLoan ~ sub_grade + dti + loan_amnt,
             family = "binomial", data = training)
```

Next we try to find a threshold for the binary decision

```{r}
predicted <- predict(model, newdata = training,type="response")
```


```{r message = FALSE}
if (!require(ROCR)) {
  install.packages("ROCR")
  library(ROCR)
}
pred <- prediction(predicted, training$isBadLoan)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, colorize=T, print.cutoffs.at=seq(0,1,by=0.1))
```

We can see the trade off between type I and type II errors here: as we make out cutoff closer to 0, we reject more good loans, but also make fewer mistakes on bad loans, hence higher sensitivity, but higher false positive rate as well.

If we were to weigh false positives and false negative equally and only look at the overall classification accuracy, then all incoming applications should be approved.

```{r}
acc.perf = performance(pred, measure = "acc")
plot(acc.perf)
```

0/1 loss weighs misclassification in two classes equally, and leads to degenerate models, or models that favor the majority group overwhelmingly. Vanilla versions of logistic regression, classification trees, etc all do with equal weights, and suffer from the same problem.

A misclassified bad loan is much more costly than a misclassified good loan in practice; underwriter should be more conservative.

Let's take into account the costs and find optimal threshold that minimizes the total cost.

```{r}
tpr <- slot(perf, "y.values")[[1]]
fnr <- 1 - tpr
fpr <- slot(perf, "x.values")[[1]]
cut.offs <- slot(perf, "alpha.values")[[1]]

number.positive <- sum(training$isBadLoan)
number.negative <- length(training$isBadLoan) - number.positive

type.I.error.cost <- 1000
type.II.error.cost <- 5000

total.cost <- type.I.error.cost * number.negative * fpr +
                    type.II.error.cost * number.positive * fnr
average.cost <- total.cost / nrow(training)
opt.cut.off <- cut.offs[which.min(average.cost)]

plot(cut.offs, average.cost, type = 'l',
     xlab = "Cutoff", ylab = "Average cost")
points(opt.cut.off, min(average.cost), pch = 5, lwd = 2, col = 4)
text(opt.cut.off, min(average.cost),
     labels = paste0("optimal cut-off: ", round(opt.cut.off, digits = 3),
                    ", with cost: $", round(min(average.cost), digits = 2),
                    " on training"),
     pos = 4)
```

Now we can test our model using the testing data

```{r}
predicted.testing <- predict(model, newdata = testing, type="response") > opt.cut.off
predicted.testing <- as.numeric(predicted.testing)
type.I.error <- sum(!(testing$isBadLoan) & predicted.testing)
type.II.error <- sum((testing$isBadLoan) & !predicted.testing)

total.cost <- type.I.error.cost * type.I.error +
                    type.II.error.cost * type.II.error
(average.cost <- total.cost / nrow(testing))
```


## Putting everything together

We have done model fitting and evaluation on one split of the data. Now we can repeat the procedure for the rest of the 5 folds.

```{r}
CV <- function(k, K, model.formula) {
  fold.size <- nrow(LCdata) / K
  testing.index <- (k-1)*fold.size + 1:fold.size
  
  training <- LCdata[-testing.index,]
  testing <- LCdata[testing.index,]
  
  model <- glm(formula = model.formula,
               family = "binomial", data = training)
  predicted <- predict(model, newdata = training,type="response")
  pred <- prediction(predicted, training$isBadLoan)
  perf <- performance(pred, measure = "tpr", x.measure = "fpr")
  
  tpr <- slot(perf, "y.values")[[1]]
  fnr <- 1 - tpr
  fpr <- slot(perf, "x.values")[[1]]
  cut.offs <- slot(perf, "alpha.values")[[1]]
  
  number.positive <- sum(training$isBadLoan)
  number.negative <- length(training$isBadLoan) - number.positive
  
  type.I.error.cost <- 1000
  type.II.error.cost <- 5000
  
  total.cost <- type.I.error.cost * number.negative * fpr +
                      type.II.error.cost * number.positive * fnr
  average.cost <- total.cost / nrow(training)
  opt.cut.off <- cut.offs[which.min(average.cost)]
  
  predicted.testing <- predict(model, newdata = testing, type="response") > opt.cut.off
  predicted.testing <- as.numeric(predicted.testing)
  type.I.error <- sum(!(testing$isBadLoan) & predicted.testing)
  type.II.error <- sum((testing$isBadLoan) & !predicted.testing)

  total.cost <- type.I.error.cost * type.I.error +
                      type.II.error.cost * type.II.error
  return(average.cost <- total.cost / nrow(testing))
}
```

Record performance on testing dataset

```{r}
# performance storage
average.cost.testing <- numeric(K)
for (k in 1:K) {
  average.cost.testing[k] <- CV(k = k, K = K, 
                                model.formula = isBadLoan ~ sub_grade + dti + loan_amnt)
}
average.cost.testing
# final performance
mean(average.cost.testing)
```

Let us compare this with a larger model, with `home_ownership` included

```{r}
# performance storage
average.cost.testing <- numeric(K)
for (k in 1:K) {
  average.cost.testing[k] <- CV(k = k, K = K,
                                model.formula = isBadLoan ~ sub_grade + dti + loan_amnt + home_ownership)
}
average.cost.testing
# final performance
mean(average.cost.testing)
```

The performance seems comparable. Adding new variables to the logistic regression, does not always decrease your CV cost/error, as we previously explained.

There may also be better ways to model the relationship, consider taking transformations of the vairables (log, squared, etc.).

.