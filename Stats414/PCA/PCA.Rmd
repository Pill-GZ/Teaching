---
title: "A Tutorial on Pincipal Component Analysis"
author: 'GAO Zheng' 
date: "Feb 2, 2017" 
output:
  html_document: 
    number_sections: true
    css: "~/Teaching/width.css"
  html_notebook: default
---

This is a tutorial on applied principal component analysis. Minimal background on linear algebra is assumed.
Notations roughly follow that of *Applied Multivariate Statistical Analysis* by R.A. Johndon and D.W.Wichern.
Interactive display plotted with [Plotly](https://plot.ly).

For derivations of the principal components, refer to [this page](./PCA_derivations.html)


```{r, include = F}
if (!require("rgl")){
  install.packages("rgl")
  library(rgl)
}
if (!require("car")){
  install.packages("car")
  library(car)
}
if (!require("plotly")){
  install.packages("plotly")
  library(plotly)
}
if (!require("printr")){
  install.packages(
    'printr',
    type = 'source',
    repos = c('http://yihui.name/xran', 'http://cran.rstudio.com')
  )
  library(printr)
}


```

# A motivating example for data reduction

`mtcars` is a data frame with 32 observations on 11 variables. The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). ^[Henderson and Velleman (1981), Building multiple regression models interactively. *Biometrics*, **37**, 391–411.]

Its columns are

- `mpg`:	 Miles/(US) gallon
- `cyl`:	 Number of cylinders
- `disp`:	 Displacement (cu.in.)
- `hp`:	 Gross horsepower
- `drat`:	 Rear axle ratio
- `wt`:	 Weight (1000 lbs)
- `qsec`:	 1/4 mile time
- `vs`:	 V/S
- `am`:	 Transmission (0 = automatic, 1 = manual)
- `gear`:	 Number of forward gears
- `carb`:	 Number of carburetors


We will be working with only three of the columns: cylinder displacement, gross horsepower, and vihecle weight.


```{r}
mtcars3col <- mtcars[,c("disp","hp","wt")]
summary(mtcars3col)
```

We color the plot by transmission type. The plot is interactive, try give it a spin to see the data is 3D. (This may not work on your mobile phone.)

```{r}
mtcars$am[which(mtcars$am == 0)] <- 'Automatic'
mtcars$am[which(mtcars$am == 1)] <- 'Manual'
mtcars$am <- as.factor(mtcars$am)


p <- plot_ly(mtcars, x = ~hp, y = ~wt, z = ~disp, color = ~am, colors = c('#BF382A', '#0C4B8E')) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'Gross horsepower'),
                     yaxis = list(title = 'Weight'),
                     zaxis = list(title = 'Displacement')))
p
```


It seems that most of the vairation is along a single direction (or two, depending on how sharp your eyes are). This phenomenon happens in many situations: in datasets with many vairiables (large p), the data points resides almost in a linear subspace of a much smaller dimension.

It is natural to ask:

1. Is there a way to represent the dataset is a way that is consistent with this observation? i.e., most of the variation happens in one (or a few) of the variables.

2. Is there a way, using linear combinations of the variables to represent the data without losing much information, so that we can work with fewer variables?


# PCA as a data reduction tool

We next introduce principal component analysis as a tool for data reduction, and describe the idea heuristically.

We have make precise what we mean by "summarize data in a few variables" and "not losing much information". 

Suppose we have a p-dimensional random vector $\mathbf{X}' = (X_1, X_2, \dots , X_p)$, we represent each data point as a collection o flinear combinations $Y$'s of its original data $X$'s.

$$
Y_1 = a_1'X = a_{11}X_1 + \dots + a_{1p}X_p\\
Y_2 = a_2'X = a_{21}X_1 + \dots + a_{2p}X_p\\
\dots\\
Y_p = a_p'X = a_{p1}X_1 + \dots + a_{pp}X_p\\
$$

In compact form, 

$$
Y = AX
$$

where $A$ is a $p$ by $p$ matrix; $i^{th}$ row of A is $a_i'$.

We want most of the variation in the data $X$ to be summarized into as few components of $Y$'s as possible. Intuitively, we want variance of $Y_1$ to be as large as possible, and the variances of the rest of the $Y$'s in decreasing order.

A moments thought reveals that this cannont be done without contraints on $A$:

1. Scaling $a_1$ by a constant factor increase the variance of $Y_1$, leading to indeterminancy. We require that $a_i'a_i = 1$, i.e., $a_i$'s are unit vectors.

2. We also require that whatever has been explained in earlier $Y$'s does not re-enter later $Y$'s. We require $a_i'a_j = 0$, i.e., $a_i$ is orthogonal to $a_j$ whenever $i\ne j$. (It is in fact equivalent to require that the components of $Y$ are independent; left as an exercise).

The coefficients of the linear combinations therefore has the interpretation of a 'direction'; each is direction orthogonal to the rest.

In two dimensions this is illustrated with the folloing plot. The two directions of linear combinations should be perpendicular to each other, with the first maximizing the variance of data along its direction.

```{r, echo=FALSE, message=FALSE}
Sigma <- 0.5*matrix(c(2,1,1,1),2,2)
V <- -eigen(Sigma)$vectors


library(MASS)
X <- mvrnorm(300, c(0,0), Sigma)
X <- data.frame(X)
names(X) <- c("x1", "x2")
X %>% 
  plot_ly(x = ~x1, y = ~x2,
          name = "sample",
          type = 'scatter',
          color = I("lightgreen"),
          mode = "markers") %>%
  add_lines(x = c(0,V[1,1]), y = c(0,V[2,1]),
            name = "PC 1",
            line = list(color = 'rgb(22, 96, 167)', width = 4, dash = 'dash')) %>%
  add_lines(x = c(0,V[1,2]), y = c(0,V[2,2]),
            name = "PC 2",
            line = list(color = 'rgb(22, 96, 167)', width = 4, dash = 'dot')) %>%
#  add_annotations(x = V[1,], y = V[2,],
#                  text = "", xref = "x", yref = "y",
#                   showarrow = TRUE, arrowcolor = I("red"),
#                   arrowhead = 7, arrowsize = .5,
#                   axref = "x", ax = 0, ayref = "y", ay = 0, 
#                   color = I("red") )%>%
  layout(showlegend = TRUE,
         xaxis = list(range = c(-3, 3)),
         yaxis = list(range = c(-2.5, 2.5))
  )
```

In the context of the example above, we try to find 'directions' along which variability in the data is maximized. The data can be summarized as a 'score', which is calculated as a linear combination of the original variables in that direction.

Mathematically, this 'direction' can be estimated by the eigenvector corresponding to the largest eigenvalue of the sample covariance matrix of your data.

# Back to the example

Lets estimate principal components of the car data that we introduced earlier.

```{r}
centered <- scale(mtcars3col, center = T, scale = F)
S <- t(centered) %*% centered / (nrow(centered) - 1)
eigen(S)
```

R has a built-in function `prcomp` in `stats` package that does this automatically. 

```{r}
(mtcarsPCA <- prcomp(mtcars3col))
```

The promised scree plot:

```{r fig.width=6,fig.height=4}
par(mar = c(2,4,1,1))
plot(mtcarsPCA$sdev^2, type = 'b')
```

first component explained `r mtcarsPCA$sdev[1]^2/sum(mtcarsPCA$sdev^2)` of the variations in the data, consistent with our initial observarion that variations in a single direction is dominating.

## PC as projections to lower-dim spaces

The implication of the above finding is that one can represent the data using fewer dimensions without losing much of the information.

In the `mtcars` example one could potentially use a single number that is the linear combination of cylinder displacement, gross horsepower, and vihecle weight for each data point, and achieve a 67% saving in terms of storage (store 1 variable instead of 3). 

The 1st PC-score for the $i^{th}$ obsercation is

$$
\begin{aligned}
\text{PC}_{i,1} & = v_1' (x_1 - \bar{x}) \\
& = 0.900 * (\text{disp}-\bar{\text{disp}}) + 0.435 * (\text{hp} - \bar{\text{hp}}) + 0.006 * (\text{wt} - \bar{\text{wt}})
\end{aligned}
$$

To recover the original variables approximately using only the first PC-score, 

$$
\begin{bmatrix}\text{disp}\\\text{hp}\\\text{wt}\end{bmatrix}_i 
\approx \text{PC}_{i,1}\hat v_1 + 
  \begin{bmatrix}\bar{\text{disp}}\\\bar{\text{hp}}\\\bar{\text{wt}}\end{bmatrix}
= \text{PC}_{i,1} \begin{bmatrix}0.900\\0.435\\0.006\end{bmatrix}  + 
  \begin{bmatrix}\bar{\text{disp}}\\\bar{\text{hp}}\\\bar{\text{wt}}\end{bmatrix}
$$


## PCA on standardized data

The PC loading of the first component on the third variable in this example is extremely small compared to the other two coordinates. This is an artifact of the difference in unit of measurement in the three variables, and the lack of standardization before we performed the procedure. 

The measurements in the variables are by nature orders of magnitudes apart. If we choose to preserve the overall distance structure of the dataset, the measurements that are smaller are naturally disadvantaged. In recovery the relative errors are larger on variables originally on smaller scales.

To correct for this artificial preference, we can perform PCA on the standardized dataset.

```{r}
(mtcarsPCAstd <- prcomp(mtcars3col, scale. = T))
```

Now the first component explained `r mtcarsPCAstd$sdev[1]^2/sum(mtcarsPCAstd$sdev^2)` of the variations in the data, but the loadings are much more comparable.

In practice, standarzation is always a good idea, unless units/special meanings are to be preserved.

One may want to think about if it even makes sense to PCA on variables with different units for interpretation. (What does it mean when we add, say distance^2 to seconds^2?)


## Number of principal components

Number of principal component to use can be decided by looking at the scree plot. We look for an 'elbow' in the plot, where much of the variation in the data are contained in the PC's that come before the 'elbow'.

In the example above the kink seems to be at 2; second and third PC are responsible for little vairability in the data.

If we perform the same analysis on the first 7 columns of `mtcars`

```{r}
mtcars7PCA <- prcomp(mtcars[,1:7], scale. = T)
plot(mtcars7PCA$sdev^2, type = 'b', ylab = "Sigma^2 of PC on 7 columns")
```

You may decide that the elbow is at 3, and chooses the first 2 PC's to summarize the data.

Using more PC's can more faithfully represent and recover the data, but at the same time defeats the purpose of data reduction. One should choose this number to his/her satisfaction; the heuristic of identifying a kink in the elbow plot is a viable solution.

## Interpretation

As we have seen earlier, principal components are nothing but weighted averages of the original variables.

In the 2D synthetic data example above, the first PC indicates a positive correlation between the two variables, with most variations taking place along its directions. Scales of the standard deviations gives us a sense of how much the variation is accounted for.

In the 3D real data example (for the standardized version), the first PC $(0.605, 0.549, 0.576)$ also points to a positive quadrant in the direction of the largest variation, indicating for cars higher horse powers are assciated with heavier weight and larger engine displacement. This PC score may be interpreted as a measure of overall power of the particular model.

Second PC indicates a negative relationship between horsepower and the other two variables, after accounting for the co-movement described in the first PC. This PC is a little harder to interpret.

## Practical issues and questions

What to do when there is categorical / ordinal variable in the data?

If your goal is data reduction, PCA may not be the best tool.

If you insist on doing PCA, there is nothing wrong with preceeding as usual. However, the categorical / ordinal variables will most likely be lost in the recovery. Interpretation isn't clear either. 

One get-around may to partition data by the levels of the categorical / ordinal variables, and perform PCA on each partition. But you would have more PC directions (for each partition) to keep track of; not a pretty solution.


# Q&A


